{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0970632c-db95-4156-9ef7-331bfb244c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bbb540c6-cae9-47cb-8545-2f324a10c0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine= create_engine(\"mysql+pymysql://root:password@username/database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2e6b1193-df11-4454-ab88-9f3f7f80526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path={\n",
    "    'Customer': 'Customer.csv',\n",
    "    'Employee': 'Employee.csv',\n",
    "    'OrderDetails': 'OrderDetails.csv',\n",
    "    'Orders': 'Orders.csv',\n",
    "    'Product': 'Product.csv',\n",
    "    'Region': 'Region.csv',\n",
    "    'Warehouse': 'Warehouse.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecca4045-3f80-4712-8658-9e86c4f541a5",
   "metadata": {},
   "source": [
    "**Extract**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "77bb8be9-9b23-407c-bc67-00efef712b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data():\n",
    "    DFS= {name: pd.read_csv(path) for name, path in csv_path.items()}\n",
    "    return DFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d1f4232f-09fd-4eb6-abfe-23ead63dbf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dictionary's Keys is used in dropping rows of primary key's column that has NaN. \n",
    "primary_keys = {\n",
    "    'Customer': ['CustomerID'],\n",
    "    'Employee': ['EmployeeID'],\n",
    "    'Product': ['ProductID'],\n",
    "    'Orders': ['OrderID'],\n",
    "    'OrderDetails': ['OrderID', 'ProductID'],  # Composite key\n",
    "    'Region': ['RegionID'],\n",
    "    'Warehouse': ['WarehouseID']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80776937-8f10-488b-b155-4b4081a9e04f",
   "metadata": {},
   "source": [
    "**Transform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7d850ea3-a62f-431e-a0a0-9f213114a323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation(DFS):\n",
    "    DFS[\"Orders\"][\"OrderDate\"] = pd.to_datetime(DFS[\"Orders\"][\"OrderDate\"], format='%d-%m-%y')\n",
    "    DFS[\"Employee\"][\"EmployeeHireDate\"] = pd.to_datetime(DFS[\"Employee\"][\"EmployeeHireDate\"], format='%d-%m-%y')\n",
    "    DFS[\"OrderDetails\"][\"Totalamount\"] = DFS[\"OrderDetails\"]['OrderItemQuantity'] * DFS[\"OrderDetails\"]['PerUnitPrice']\n",
    "    DFS[\"OrderDetails\"].insert(4, \"Totalamount\", DFS['OrderDetails'].pop('Totalamount'))\n",
    "    DFS[\"OrderDetails\"].drop([\"OrderDetailsID\"], axis=1, inplace=True)\n",
    "\n",
    "    for tabName, pk_col in primary_keys.items():\n",
    "        before = len(DFS[tabName]) #Number of rows in the table before dropping\n",
    "        DFS[tabName].dropna(subset=pk_col, inplace=True)\n",
    "        after = len(DFS[tabName]) #Number of rows in the table after dropping\n",
    "        # print(f\"{tabName}: Dropped {before - after} rows due to NaN in primary key(s): {pk_col}\")\n",
    "\n",
    "   \n",
    "    for tab_name, df in DFS.items():\n",
    "        #print(df.iloc[:,0:1])\n",
    "        for col in df.columns:\n",
    "            if df[col].dtypes == object and df[col].isna().any():\n",
    "                df[col].fillna(\"Unknown\", inplace =True)\n",
    "                # print(\"Filled for Object\")\n",
    "            elif pd.api.types.is_numeric_dtype(df[col]) and df[col].isna().any():\n",
    "                df[col].fillna(0, inplace =True)\n",
    "                # print(\"Filled for Numericals\")\n",
    "            elif pd.api.types.is_datetime64_any_dtype(df[col]) and df[col].isna().any():\n",
    "                df[col].fillna(pd.Timestamp(\"1900-01-01\"), inplace =True)\n",
    "                # print(\"Filled for Datetime\")\n",
    "            else:\n",
    "                print(\"No NaN\")\n",
    "\n",
    "    # ETL Load Order (Dimension and Fact Tables)\n",
    "    tables = {\n",
    "        \"dim_region\": DFS['Region'],\n",
    "        \"dim_warehouse\": DFS['Warehouse'],\n",
    "        \"dim_customer\": DFS['Customer'],\n",
    "        \"dim_product\": DFS['Product'],\n",
    "        \"dim_employee\": DFS['Employee'],\n",
    "        \"dim_orders\": DFS['Orders'],\n",
    "        \"fact_sales\": DFS['OrderDetails']\n",
    "    }     \n",
    "    \n",
    "    return tables\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99bd158-9507-4760-9fdf-5748fe813436",
   "metadata": {},
   "source": [
    "**Duplicate removal using PK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "25697221-c09f-4fd5-adba-90f4f148effb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dictionary's Keys (table names) is mapped/named based on the names from \"tables\" dict in def transformation function\n",
    "primary_keys_rem_dup = {\n",
    "    \"dim_region\": ['RegionID'],\n",
    "    \"dim_warehouse\": ['WarehouseID'],\n",
    "    \"dim_customer\": ['CustomerID'],\n",
    "    \"dim_product\": ['ProductID'],\n",
    "    \"dim_employee\": ['EmployeeID'],\n",
    "    \"dim_orders\": ['OrderID'],\n",
    "    \"fact_sales\": ['OrderID','ProductID'] #Composite Primary Key\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fd0a401f-1dde-41b8-8685-f0f64ff16a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(tables):\n",
    "    Updated_tables={}\n",
    "    \n",
    "    for table_name, DFS in tables.items():\n",
    "        # Fetch only the primary key columns from SQL table\n",
    "        pk_cols_list=primary_keys_rem_dup.get(table_name)\n",
    "        # Optional in-case\n",
    "        # if not pk_cols_list:\n",
    "        #     print(f\"No primary key defined for {table_name}, skipping duplicate check.\")\n",
    "        #     updated_tables[table_name] = df\n",
    "        #     continue\n",
    "\n",
    "        # Converts the primary key list into a comma-separated string usin ','.join for use in the SQL query.\n",
    "        pk_cols_str= ','.join(pk_cols_list)\n",
    "        query = f\"Select {pk_cols_str} from {table_name}\" # Queries the existing primary key values from the SQL table.\n",
    "        DFS_exists = pd.read_sql(query, engine)  #Loads the result into a DataFrame called df_exists.\n",
    "\n",
    "        # Find rows in df that are not in df_exists\n",
    "        DFS_combined=DFS.merge(DFS_exists, on=pk_cols_list, how= \"left\", indicator = True)\n",
    "        \n",
    "        # Filter the merged DataFrame to keep only new rows (left_only) and Remove the _merge column.\n",
    "        DFS_new= DFS_combined[DFS_combined[\"_merge\"] == \"left_only\"].drop(columns=[\"_merge\"])\n",
    "        # print(f\"{table_name}: {len(DFS) - len(DFS_new)} duplicates removed. {len(DFS_new)} new rows added.\")\n",
    "        if (len(DFS) - len(DFS_new)) == len(DFS):\n",
    "            print(f\"No duplicates in the {table_name}\")\n",
    "        else:\n",
    "            print(f\"{len(DFS) - len(DFS_new)} duplicates is removed. {len(DFS_new)} new rows added.\")\n",
    "\n",
    "        Updated_tables[table_name]= DFS_new # Stores the cleaned DataFrame in the Updated_tables dictionary.\n",
    "        \n",
    "    return Updated_tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0143534f-695a-4a38-aabc-bccc4f3c2079",
   "metadata": {},
   "source": [
    "**Load**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7e51bb85-d475-473a-8bf3-192703f9379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_SQL(Updated_tables):\n",
    "    with engine.begin() as conn:\n",
    "        for table_name, DFS in Updated_tables.items():\n",
    "            print(f\"Loading {table_name}\")\n",
    "            DFS.to_sql(name=table_name, con=conn, if_exists='append', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7c286110-3e2b-481d-b6c5-e63166cc1971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No NaN\n",
      "No duplicates in the dim_region\n",
      "No duplicates in the dim_warehouse\n",
      "No duplicates in the dim_customer\n",
      "No duplicates in the dim_product\n",
      "No duplicates in the dim_employee\n",
      "No duplicates in the dim_orders\n",
      "No duplicates in the fact_sales\n",
      "Loading dim_region\n",
      "Loading dim_warehouse\n",
      "Loading dim_customer\n",
      "Loading dim_product\n",
      "Loading dim_employee\n",
      "Loading dim_orders\n",
      "Loading fact_sales\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": #Only run this block of code if this file is being run directly (not imported).\n",
    "    extracted_data = extract_data()\n",
    "    transformed_data = transformation(extracted_data)\n",
    "    clean_data=remove_duplicates(transformed_data)\n",
    "    load_to_SQL(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade63e6d-acc1-4824-868c-4df8b2b30c72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291257cc-1685-4bef-9abc-04baaede6066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a589d7ee-52aa-43a4-82ad-09910ed697f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
